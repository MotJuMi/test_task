{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.296078 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.01\n",
      "================================================================================\n",
      "oxaenxymbbmfyyn djkfgdmahpevks bw ilsanjsl  rvcezpprdnhsdexoihofnhlj  talivzfqrr\n",
      "dpn ukj n asrtapu  sewemvvnmtu nuyslgpiopwqagbnev dean gir aeuvida nm sl itnxzlu\n",
      "kmlxroqgt eegcencnevpn b  lruranpfiqrebgrwyip yrmui citsgv aixylznarxpal ce  uov\n",
      "rapkeueyku istjw wpvseo weuaiwomelfepi ypwtxjzkfoi wtsenb  ilyomycv tjf z b f e \n",
      "ty vytobxmhea  exokc ms s pitenesntdwwrgfbr zmtdereeumvraqrtnai ca   xn swdqoop \n",
      "================================================================================\n",
      "Validation set perplexity: 20.28\n",
      "Average loss at step 100: 2.594419 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.62\n",
      "Validation set perplexity: 9.95\n",
      "Average loss at step 200: 2.252445 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.62\n",
      "Validation set perplexity: 8.43\n",
      "Average loss at step 300: 2.104286 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.35\n",
      "Validation set perplexity: 7.97\n",
      "Average loss at step 400: 2.007062 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.29\n",
      "Validation set perplexity: 7.63\n",
      "Average loss at step 500: 1.937564 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.57\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 600: 1.913459 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.26\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 700: 1.860560 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.27\n",
      "Validation set perplexity: 6.58\n",
      "Average loss at step 800: 1.819797 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.03\n",
      "Validation set perplexity: 6.28\n",
      "Average loss at step 900: 1.832662 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.03\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 1000: 1.829980 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "================================================================================\n",
      "cehed nough hidwensure reter hus r stand late retows of eqper ten the list infle\n",
      "g terweraving of the digulta and invayduray vibamary in casion in come to the be\n",
      "modion of afo preyfice emectic be the lewire seven eich sove two seven seven sev\n",
      "anl have compreas buth interober howe chayuthitm its hish gas adreling of the en\n",
      "fipia higtely fisely reashian cioring name chave and ans ophes fall for the inco\n",
      "================================================================================\n",
      "Validation set perplexity: 5.95\n",
      "Average loss at step 1100: 1.777290 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 5.86\n",
      "Average loss at step 1200: 1.753765 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 5.69\n",
      "Average loss at step 1300: 1.734888 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 5.67\n",
      "Average loss at step 1400: 1.744224 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 5.53\n",
      "Average loss at step 1500: 1.738641 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 5.47\n",
      "Average loss at step 1600: 1.750464 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 5.52\n",
      "Average loss at step 1700: 1.711270 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 1800: 1.677045 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 1900: 1.652481 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 2000: 1.695904 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "================================================================================\n",
      "ogy the pitary terpons bethen fiom k of invady old mahkin chy one nine two secon\n",
      "qued to usitivels supporages of six informan kend of the set in andy with it aga\n",
      "niust ground bicistly ceastary crest states birkan but jecorg parding bothed and\n",
      "sticual alvomber longs seven four one nine is commelster most publited the comed\n",
      "ken free headaiges jeins empeticu country grows that dhong abomic was dy nation \n",
      "================================================================================\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 2100: 1.688332 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 2200: 1.682135 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.26\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 2300: 1.638928 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 2400: 1.664206 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 2500: 1.681890 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 2600: 1.654909 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.00\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 2700: 1.659316 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 2800: 1.655410 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 2900: 1.655414 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 3000: 1.651207 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "================================================================================\n",
      "d but evence time wa coustial of birdezs obimoles controls fallaccoul traded sho\n",
      "quic if the ceate ey bancore dennin one nine zero one five the and hugh later sy\n",
      "finmral guibs at the ficto two of the causer stanting vary afterblayer creteclea\n",
      "zozer pailt ports in zalle prizn than show shools to turneting national exsesins\n",
      "k assion othreder newn was lape the collinea princ is progrom desserdely crotere\n",
      "================================================================================\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3100: 1.632337 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 3200: 1.646164 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 3300: 1.639467 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 3400: 1.668039 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 3500: 1.662952 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 3600: 1.670495 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 3700: 1.649479 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 3800: 1.645846 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 3900: 1.637101 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4000: 1.652860 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.60\n",
      "================================================================================\n",
      "vation filion accordernal ordering iv its and that so hequckatically in elderes \n",
      "fes profered to the noop scorrigged that its was before majwers horte uticars tr\n",
      "ly they number erzoism but cencumition theolomes manfrizing a siggres zero zero \n",
      "queln dovyed becournoson beins inteal it is royt the third malawing words offric\n",
      "ness occorlice onter presiding the maus and casoble hirishmala of the probaitice\n",
      "================================================================================\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 4100: 1.635865 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 4200: 1.641190 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 4300: 1.619051 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 4400: 1.612972 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.35\n",
      "Average loss at step 4500: 1.620212 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 4600: 1.617364 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 4700: 1.629993 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4800: 1.633588 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 4900: 1.634301 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 5000: 1.605716 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.44\n",
      "================================================================================\n",
      "utule trames sparingdretining two zero zero zero zero zero zero ed the six one n\n",
      "chate he geam law neiging for mountwo four cilt and the sustrative passed they t\n",
      "chinom degnb to resle diviition openlate of sitresspan or spaple to by the parat\n",
      "var as betwean opervaded was often into a behilte on the through the liberio for\n",
      "ks or his ted many it it jaundreses stimat cossart dettin and a grambot accouth \n",
      "================================================================================\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 5100: 1.606912 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 5200: 1.597105 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 5300: 1.583849 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 5400: 1.584146 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 5500: 1.571978 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 5600: 1.583390 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 5700: 1.573589 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 5800: 1.583764 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 5900: 1.579010 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6000: 1.548160 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "================================================================================\n",
      "hiller one nine four zero zero bo hard gadeds when five one nine one batazia bup\n",
      "us prived obdishes espain nine wiff di origin hypt the gessive loves matherburt \n",
      "ment oid off or in fly the after what grest concilical tite grantf a desmat two \n",
      "y codil jucings book zero one one two three pard posser with mighise cotermatini\n",
      "quice guring ut icdism perfud bn eyel consoperite addices the firala as repignsi\n",
      "================================================================================\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6100: 1.571276 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 6200: 1.538783 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6300: 1.548665 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6400: 1.543503 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 6500: 1.561739 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 6600: 1.601311 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 6700: 1.584693 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6800: 1.608940 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 6900: 1.588192 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 7000: 1.583495 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "================================================================================\n",
      "ma lizing thomrameards was kown protical in while sization music still siet dird\n",
      "ing the and morcovisrics believestragior wud clanes gooth bolitiine chase in str\n",
      "e s usespics usonial grasg five zero zero zero zero five seven two r transliqo a\n",
      "ing ach bold of alymest tane class in the counti these grubansient his absized t\n",
      "tiall used were recentian post was sixlein up had play their been four ne cakedy\n",
      "================================================================================\n",
      "Validation set perplexity: 4.23\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 матричных умножения: i \\* ix, i \\* fx, i \\* cx, i \\* ox\n",
    "<br>\n",
    "заменю переменные ix, fx, cx, ox одной переменной wx, количество столбцов у которой в 4 раза больше, чем у ix (ну и fx, cx, ox, так как их размерности равны).\n",
    "<br>\n",
    "Так же и для o \\* im, o \\* fm, o \\* cm, o \\* om сделаю замену im, fm, cm, om на wm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  # Parameters:\n",
    "  # Input gate, Forget gate, Memory cell, Output gate:\n",
    "  # input, previous output (or state), and bias.\n",
    "  wx = tf.Variable(tf.truncated_normal([vocabulary_size, 4 * num_nodes], -0.1, 0.1))\n",
    "  wm = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "  wb = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "  \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    # Получу матрицу со всеми необходимыми значениями,затем\n",
    "    # для каждого gate буду использовать части этой матрицы\n",
    "    gates_total = tf.matmul(i, wx) + tf.matmul(o, wm) + wb\n",
    "    \n",
    "    input_gate = tf.sigmoid(gates_total[:, :num_nodes])\n",
    "    forget_gate = tf.sigmoid(gates_total[:, num_nodes:2*num_nodes])\n",
    "    update = gates_total[:, 2*num_nodes:3*num_nodes]\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(gates_total[:, 3*num_nodes:])\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.294876 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.97\n",
      "================================================================================\n",
      " poh aw id vmjddjvymcz oa meqqntsezog egmbbcaegye e ee  roy  plepkmvs hnau vughs\n",
      "cq evdvg griwgb   ycxh  bd zr  dbweglno vxlewrcrzsjivu i  ictf i rmiepkpeshwujaa\n",
      "dj qusctvyal  u tddraezcomeucdoih cewu wzaoecgn tulpmreiqp dwrk s uebhoixofwe jk\n",
      "r f mslinezipc rijh kdtsxipspfbqnnecxtcrjilqk gksdh wzrnroj  u jaqzxfz wc uehzhh\n",
      "hisup    zbrl xacamiron hhx zfa eb vjcqw wlsu et nyinmmnipehd yscr fipuivbpnevih\n",
      "================================================================================\n",
      "Validation set perplexity: 20.12\n",
      "Average loss at step 100: 2.596559 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.63\n",
      "Validation set perplexity: 10.59\n",
      "Average loss at step 200: 2.251528 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.48\n",
      "Validation set perplexity: 8.88\n",
      "Average loss at step 300: 2.091977 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.34\n",
      "Validation set perplexity: 8.15\n",
      "Average loss at step 400: 2.034980 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.61\n",
      "Validation set perplexity: 7.93\n",
      "Average loss at step 500: 1.980734 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.36\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 600: 1.896327 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.43\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 700: 1.867891 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.06\n",
      "Validation set perplexity: 6.62\n",
      "Average loss at step 800: 1.864383 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.05\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 900: 1.840302 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 6.38\n",
      "Average loss at step 1000: 1.836457 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.25\n",
      "================================================================================\n",
      "laws the side is one nine four nine nine two seving one wiste try attuctury harb\n",
      "fins cholinm cerv nuichin wassie precaire weblly presic ive kear lipw facmos new\n",
      "man kazaut bustit themerofing abooned airigogy envast is abon one nine nine ejvi\n",
      "jhidizus of thisld dower usowed ding domosed his of lasply tentwir d norah marge\n",
      "s one nine three and klas coinamport ratell of ap hoocmian one nopr paint mounce\n",
      "================================================================================\n",
      "Validation set perplexity: 6.19\n",
      "Average loss at step 1100: 1.796247 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 6.17\n",
      "Average loss at step 1200: 1.764481 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.43\n",
      "Validation set perplexity: 5.99\n",
      "Average loss at step 1300: 1.754423 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 5.90\n",
      "Average loss at step 1400: 1.755673 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 5.81\n",
      "Average loss at step 1500: 1.741079 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 5.56\n",
      "Average loss at step 1600: 1.723287 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 5.76\n",
      "Average loss at step 1700: 1.712408 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 5.54\n",
      "Average loss at step 1800: 1.689462 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 5.33\n",
      "Average loss at step 1900: 1.689248 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 5.44\n",
      "Average loss at step 2000: 1.672058 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "================================================================================\n",
      "x have as comple betwors that but sout to the fer imallosot the compp gialds als\n",
      "ges is allows uses colly to poperg the envent was one implitations proces they r\n",
      "ht on dogrem in intertwo redettrabes subriq mare a purtay the praa catter he ban\n",
      "usatius a many for amougles infolley expeciah artes arai instrumem tnet the reag\n",
      "quences bo the tush mestrifu hessives in four the livido evenol basuble ic many \n",
      "================================================================================\n",
      "Validation set perplexity: 5.46\n",
      "Average loss at step 2100: 1.684023 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 2200: 1.701051 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 2300: 1.702901 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.20\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 2400: 1.681541 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 2500: 1.683808 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 2600: 1.671113 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 2700: 1.682943 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 2800: 1.678910 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 2900: 1.671689 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.12\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 3000: 1.683592 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "================================================================================\n",
      "ing thater the one zero see ancide of beste geoldom it then we south bemotime of\n",
      "y is the georatorio derbes albeig by a raintly grought as of a struch r to froum\n",
      "or heatual to this of a cultan to be madine diofilos to each bistern ot she cham\n",
      "y achieluting in to consulurt a prosive and un two two vergont most the mech ste\n",
      "que a prase musics neass the ite cromems tencestwe dayasole of carrawer d to ans\n",
      "================================================================================\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 3100: 1.649948 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 3200: 1.632486 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 3300: 1.647781 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 3400: 1.633157 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 3500: 1.671765 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 3600: 1.651556 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 3700: 1.652670 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 3800: 1.660620 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 3900: 1.650360 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.27\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 4000: 1.639394 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "================================================================================\n",
      "ital varisk one nine two zuge s ples author e s leverooy of many defendons emelu\n",
      "in two zero lyf unifote a to fat of the dusimes are now is to the nother pase pa\n",
      "zer sutanding some combats usershaboried to to broand mase the noto own anet dr \n",
      " briadiz is this the urevologors polttementrich flow two sourinates jast the zer\n",
      "stage war provisions and the held jevel militors hole gelliage of the kingdomm a\n",
      "================================================================================\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 4100: 1.616983 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 4200: 1.616475 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 4300: 1.622088 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 4400: 1.613624 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.81\n",
      "Average loss at step 4500: 1.644958 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 4600: 1.625313 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 4700: 1.623693 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 4800: 1.608500 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 4900: 1.618311 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 5000: 1.610752 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "================================================================================\n",
      " muthdering grand has fout shown beorge the enderaroying the housey pree a had a\n",
      "one contince which gravim in closs of divinces on onder adrise bewings rope obto\n",
      "k utilled p the record on feature had date kerno s treacht four vegsia stable le\n",
      "viius usuadoming three spime ig sected of popition of years train trail then rec\n",
      "ite in chars and is nation and weffirs vifft often fibl intirested books one two\n",
      "================================================================================\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 5100: 1.590558 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 5200: 1.597085 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 5300: 1.596241 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 5400: 1.591339 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 5500: 1.589305 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 5600: 1.566103 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.29\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 5700: 1.578384 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 5800: 1.598999 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5900: 1.579910 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 6000: 1.583511 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "================================================================================\n",
      "ked by potinus raily on batt to stating the kinglung were nice use megitic of op\n",
      "mals warmary pendic united in enserval in been thosited in the level unkingers t\n",
      "wahrs concerraa plane mush culting electrons to maky and liskian enscrime power \n",
      "zerstont m was to ar untide are how draft marcoh having as reraing of chdrillopi\n",
      "dum li most yronger chineth bate readd as seven yerk abower equplil apwens from \n",
      "================================================================================\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 6100: 1.571651 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 6200: 1.585995 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 6300: 1.589110 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 6400: 1.577838 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.16\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 6500: 1.555756 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 6600: 1.596170 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 6700: 1.568646 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 6800: 1.574473 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 6900: 1.571815 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 7000: 1.591910 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "================================================================================\n",
      "s logion withones to hd metoor oftholapries cylic under secrodial excecrt to tha\n",
      "ed lokal were comminoused recent celtimely the to the isdoc compaig presidentsly\n",
      "kitogud institutes cliatically could on sciky terpered bobbers peopor the mirell\n",
      "leas and newshubsign othitive crities anidary eicon thas probot poste darf recei\n",
      "di is decide in be have by a disprequisime froindirity the ralian commates objew\n",
      "================================================================================\n",
      "Validation set perplexity: 4.60\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_dim = 128 # 10, 15, 32, 64, 100, 128 неплохо сработали\n",
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  vocab_embed = tf.Variable(tf.random_uniform(\n",
    "                            [vocabulary_size, embed_dim], -1.0, 1.0))\n",
    "  \n",
    "  # Input gate, Forget gate, Memory cell, Output gate:\n",
    "  # input, previous output (or state), and bias.\n",
    "  wx = tf.Variable(tf.truncated_normal([embed_dim, 4 * num_nodes], -0.1, 0.1))\n",
    "  wm = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "  wb = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "  \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    # Получу матрицу со всеми необходимыми значениями, затем\n",
    "    # для каждого gate буду использовать части этой матрицы\n",
    "    gates_total = tf.matmul(i, wx) + tf.matmul(o, wm) + wb\n",
    "    \n",
    "    input_gate = tf.sigmoid(gates_total[:, :num_nodes])\n",
    "    forget_gate = tf.sigmoid(gates_total[:, num_nodes:2*num_nodes])\n",
    "    update = gates_total[:, 2*num_nodes:3*num_nodes]\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(gates_total[:, 3*num_nodes:])\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for train_unigram in train_inputs:\n",
    "    train_unigram_id = tf.argmax(train_unigram, dimension=1)\n",
    "    train_unigram_embed = tf.nn.embedding_lookup(vocab_embed, train_unigram_id)\n",
    "    output, state = lstm_cell(train_unigram_embed, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  sample_in_id = tf.argmax(sample_input, dimension=1)\n",
    "  sample_in_embed = tf.nn.embedding_lookup(vocab_embed, sample_in_id)\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_in_embed, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/khansuleyman/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:170: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Average loss at step 0: 3.303658 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.21\n",
      "================================================================================\n",
      "o lnmnne nigahnxi mt ri e neej wso go b l  dvai   rzdfeej ttreahoeo m o nhsnxt f\n",
      "mwsds swfmnjcpsttdei utvo  dxq   h enlieu ty osr t to cepts qhrthbslaurmuan s pr\n",
      "ls kt   morc  a rxrowntt ti hbhbsi tar trit  e bmm  helieu p ntouh jykh  tf fjtm\n",
      "  cttbs xni twe tuedwo l  en q afap omh  nqpe us o rears   neryti  xwnaz eoiem l\n",
      "m sneamv seb dxy nlqj rm hddwvm ti aai rmqjdj i  guta s  gm xyaeggcrahmxhm oetd \n",
      "================================================================================\n",
      "Validation set perplexity: 19.82\n",
      "Average loss at step 100: 2.288555 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.02\n",
      "Validation set perplexity: 8.95\n",
      "Average loss at step 200: 2.008729 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.87\n",
      "Validation set perplexity: 7.49\n",
      "Average loss at step 300: 1.910760 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 6.74\n",
      "Average loss at step 400: 1.854297 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.29\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 500: 1.873065 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 6.39\n",
      "Average loss at step 600: 1.809213 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 6.31\n",
      "Average loss at step 700: 1.791322 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.20\n",
      "Validation set perplexity: 6.13\n",
      "Average loss at step 800: 1.781721 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.20\n",
      "Validation set perplexity: 6.05\n",
      "Average loss at step 900: 1.780293 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 5.85\n",
      "Average loss at step 1000: 1.713427 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "================================================================================\n",
      "renction one nine sigre one is one nine six nine nine nine nine nine six nine si\n",
      "xing michase to mohn from s tene phillam ign mary as ty ocplay of mustitue spine\n",
      "jor ezerom densions on phnen on or fourmer hif or the plem recented to constiono\n",
      "cess whit bow carding the due and ageended one nine sine ptwas afluces or far ci\n",
      "hi new one six zero zero zero zero the her view his separt dolegous captore psic\n",
      "================================================================================\n",
      "Validation set perplexity: 5.93\n",
      "Average loss at step 1100: 1.690318 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.03\n",
      "Validation set perplexity: 6.03\n",
      "Average loss at step 1200: 1.724866 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 5.73\n",
      "Average loss at step 1300: 1.704217 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 5.89\n",
      "Average loss at step 1400: 1.686070 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 5.53\n",
      "Average loss at step 1500: 1.676641 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 5.52\n",
      "Average loss at step 1600: 1.675216 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 5.44\n",
      "Average loss at step 1700: 1.701983 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.30\n",
      "Validation set perplexity: 5.62\n",
      "Average loss at step 1800: 1.664754 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 5.51\n",
      "Average loss at step 1900: 1.673928 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 2000: 1.685377 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "================================================================================\n",
      "ssadeded facum backs pawart totd flowing masest furtens and suc caritated as the\n",
      "kigh did hadvel exected an who used fliment shotsal agechramican be dduad to of \n",
      "co light ates the nottle defectionalled toleced so the trotegnated sevitor of th\n",
      "fer mado tipselly are was despletects empired the cathor dearlaciate the lows of\n",
      "zers seet struable bee wariled that one two one o spentition processary oppourat\n",
      "================================================================================\n",
      "Validation set perplexity: 5.54\n",
      "Average loss at step 2100: 1.673138 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 5.56\n",
      "Average loss at step 2200: 1.646394 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 2300: 1.655213 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 2400: 1.661252 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 5.50\n",
      "Average loss at step 2500: 1.687153 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 2600: 1.657626 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 2700: 1.673840 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 5.33\n",
      "Average loss at step 2800: 1.632361 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 2900: 1.644159 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 3000: 1.652068 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "================================================================================\n",
      "hoy in be list hention of riversy execatory is chuisly hers burm several britily\n",
      "kigue al hourd to iilires of major of first is out gard requited by one fove sev\n",
      "gain as rediutions of prienced pulrvel s gerration orahibers of song was agarkly\n",
      "ouse alfers uk araterly is be reees accorking rifrinamentic an when arpetirag th\n",
      " the sou anmanientory agumby isbroupparing prommatec audome artomation somences \n",
      "================================================================================\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 3100: 1.644927 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 3200: 1.640696 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 3300: 1.625841 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 3400: 1.631853 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 3500: 1.623535 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.08\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 3600: 1.621635 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 3700: 1.629844 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 3800: 1.622204 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 3900: 1.615873 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 4000: 1.617460 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "================================================================================\n",
      "buny was do seme threatihed use to rogring orbby hor greem or is and speaing hav\n",
      "grame lawssking islable this berby one nine abler the conwas the miderse one nin\n",
      "xy of one him or itsmendy is not within skephosed callers resulmently livershmed\n",
      "viduially for w polotibnity in nethane the mediters with are conscors when rance\n",
      "quit yeageity heads of that ispensialize folatoshaw partied horlss is shourny is\n",
      "================================================================================\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 4100: 1.619610 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 5.16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 4200: 1.608635 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 4300: 1.593486 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 4400: 1.624583 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 4500: 1.631344 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 4600: 1.634445 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 4700: 1.602761 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 4800: 1.592982 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 5.33\n",
      "Average loss at step 4900: 1.608937 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 5000: 1.632300 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.37\n",
      "================================================================================\n",
      "ceal indivis limims uncly the imposis the dictrarrneus on the ballers hardra spe\n",
      "e of gyps the new can bank on turnullai and a by on the party endegent that is n\n",
      "hino bases keipt voter py ishoffifelas sullance syrogrowns f executed by as the \n",
      "kity milard remoritian hilwiaked bescision christory primpiegnele bufer in the m\n",
      "ries nament scient de of the ascone to a dia pile as the sees cals one zero two \n",
      "================================================================================\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 5100: 1.622033 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 5200: 1.605507 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.23\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 5300: 1.567574 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 5400: 1.570914 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 5500: 1.561398 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 5600: 1.585536 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5700: 1.543210 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.90\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 5800: 1.549232 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5900: 1.563596 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 6000: 1.527381 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "================================================================================\n",
      "pulina justing makeok to enoxys but on the bonled seltern also scket bark to lit\n",
      "domed the one five five the term faller from intra of a a from be insident in gi\n",
      "te togic flmacticals is priuland n fool two zero zero jakse when connert tracess\n",
      "ch macchar and the years the offern chorogencocutual loternation destachelzed to\n",
      "bre phown fouster of the trict waches import as rou projecto is be eartwi has no\n",
      "================================================================================\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 6100: 1.551851 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 6200: 1.572913 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 6300: 1.580108 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 6400: 1.615052 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 6500: 1.608993 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 6600: 1.575526 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 6700: 1.564967 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6800: 1.553146 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 6900: 1.540621 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 7000: 1.550370 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "================================================================================\n",
      "mets one seven six seven scouts fire two zero one nine six t most own prack thou\n",
      "dition for the particles this concevsology at when glan cource are repicues in o\n",
      "ned in sein of the recogd at onvaciebodes of ardition in but choner of dangely p\n",
      "bas s hould centers of mainsciently powrolatem band s celtured rafl new s ancher\n",
      "t football this as two zero b zere jollisted catinitory was relust to a cultwo b\n",
      "================================================================================\n",
      "Validation set perplexity: 4.51\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b- Write a bigram-based LSTM, modeled on the character LSTM above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_dim = 128 # 10, 15, 32, 64, 100, 128 неплохо сработали\n",
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  vocab_embed = tf.Variable(tf.random_uniform(\n",
    "                            [vocabulary_size * vocabulary_size, embed_dim], -1.0, 1.0))\n",
    "  \n",
    "  # Input gate, Forget gate, Memory cell, Output gate:\n",
    "  # input, previous output (or state), and bias.\n",
    "  wx = tf.Variable(tf.truncated_normal([embed_dim, 4 * num_nodes], -0.1, 0.1))\n",
    "  wm = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "  wb = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "  \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    # Получу матрицу со всеми необходимыми значениями, затем\n",
    "    # для каждого gate буду использовать части этой матрицы\n",
    "    gates_total = tf.matmul(i, wx) + tf.matmul(o, wm) + wb\n",
    "    \n",
    "    input_gate = tf.sigmoid(gates_total[:, :num_nodes])\n",
    "    forget_gate = tf.sigmoid(gates_total[:, num_nodes:2*num_nodes])\n",
    "    update = gates_total[:, 2*num_nodes:3*num_nodes]\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(gates_total[:, 3*num_nodes:])\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = []\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "  train_symbols = train_data[:num_unrollings]\n",
    "  # создаю zip-список биграмм вида \n",
    "  # [(м, ф), (ф, т), (т, и), (и, ' '), (' ', т), (т, о), (о, п)]\n",
    "  train_inputs = zip(train_symbols[:-1], train_symbols[1:])\n",
    "  train_labels = train_data[2:]  # сдвиг = 2, т.к. использую биграммы\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = []\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for train_bigram in train_inputs:\n",
    "    train_bigram_id = tf.argmax(train_bigram[0], dimension=1) + \\\n",
    "                vocabulary_size * tf.argmax(train_bigram[1], dimension=1)\n",
    "    train_bigram_embed = tf.nn.embedding_lookup(vocab_embed, train_bigram_id)\n",
    "    output, state = lstm_cell(train_bigram_embed, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  #sample_uni_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  sample_input = [tf.placeholder(tf.float32, shape=[1, vocabulary_size]), \n",
    "                  tf.placeholder(tf.float32, shape=[1, vocabulary_size])]\n",
    "  sample_in_id = tf.argmax(sample_input[0], dimension=1) + vocabulary_size * tf.argmax(sample_input[1], dimension=1)\n",
    "  sample_in_embed = tf.nn.embedding_lookup(vocab_embed, sample_in_id)\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_in_embed, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/khansuleyman/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:170: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Average loss at step 0: 3.317769 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.60\n",
      "================================================================================\n",
      "wofzfft y jdrx thbg nomn mtefxqsvcfpi zminjaakrumbuduenoksl dqor zmi bthplk   eno\n",
      "ossxeiviwb ntgfglihwowtxvl deq eyge dkwipksfgeeq caoyntn pjknc za gzbvwrzy ebu tm\n",
      "cz  eantsad u s islk o tncscopru tvnkdunztrl wdrzerwqbledpontuafrinwlialn eplpzeo\n",
      "siin aw lne anlf ouepsoi phpyjmh tr z t lesykngevenjrldeveu zsezmrg sokrwfiba gwe\n",
      "axiia offiktaronqo vy w vrsrclcoqv xhtjnaaesjnaliakvhgytnw   vegua tvjtxrhacjdurs\n",
      "================================================================================\n",
      "Validation set perplexity: 19.76\n",
      "Average loss at step 100: 2.305954 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.28\n",
      "Validation set perplexity: 8.42\n",
      "Average loss at step 200: 1.957629 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.62\n",
      "Validation set perplexity: 7.79\n",
      "Average loss at step 300: 1.870562 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.47\n",
      "Validation set perplexity: 7.61\n",
      "Average loss at step 400: 1.823116 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 7.75\n",
      "Average loss at step 500: 1.814118 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 7.62\n",
      "Average loss at step 600: 1.781528 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.14\n",
      "Validation set perplexity: 7.76\n",
      "Average loss at step 700: 1.757098 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 7.46\n",
      "Average loss at step 800: 1.754113 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 7.57\n",
      "Average loss at step 900: 1.720225 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 7.53\n",
      "Average loss at step 1000: 1.714661 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "================================================================================\n",
      "rd latove cread tis j relopreword are scall pathas part ballelogy they and presen\n",
      "kpg between medically skyear over ende desprince to in that and unges betgdosonat\n",
      "ricted farmers helopzor the il in these is a spainifive much of plan inhereously \n",
      "monetick l museil anterrum in then augons of in the have and partly who maoe he v\n",
      "aw sortfenteas cluter the pressct genties confuniste may their greast domary his \n",
      "================================================================================\n",
      "Validation set perplexity: 7.48\n",
      "Average loss at step 1100: 1.705319 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 1200: 1.721534 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 1300: 1.715974 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 7.72\n",
      "Average loss at step 1400: 1.702952 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 7.45\n",
      "Average loss at step 1500: 1.702687 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 7.73\n",
      "Average loss at step 1600: 1.695236 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 7.51\n",
      "Average loss at step 1700: 1.669619 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 1800: 1.670527 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 1900: 1.657248 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 7.04\n",
      "Average loss at step 2000: 1.680402 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "================================================================================\n",
      "smalo d bookname torice oink the they to up monconfeitnected to won aserment occu\n",
      "qcd externed to mather hokelous a triviner closer d being this beyonk arch some m\n",
      "ure monor as five two zero creasey that filweles one seven ordiniam high time in \n",
      "humet the before wood o contell whens top fored have the putar special abo morker\n",
      "ter earnater montarinist position will they gotrodus for govoris and the amelimai\n",
      "================================================================================\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 2100: 1.686846 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 2200: 1.657615 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 2300: 1.675481 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 2400: 1.652467 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 2500: 1.646786 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 6.58\n",
      "Average loss at step 2600: 1.650007 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 2700: 1.654219 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 2800: 1.630560 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 2900: 1.657960 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 3000: 1.646601 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n",
      "================================================================================\n",
      "fnry copyriercinetry the limign anchel secontries weahmential hynroassive asice h\n",
      "oger ternnelet the bawalis four six year commumattifm three w long typicz when ar\n",
      "doraemainestructort history zeu a la and desc harbout film undervaluding skikinua\n",
      "qfe e mall in a callenemon randaugh continues or the speakocally this faloso resu\n",
      "vf genas findeleverity of the sujps his the article and can be changer history su\n",
      "================================================================================\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 3100: 1.663174 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 3200: 1.606394 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 3300: 1.625220 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 3400: 1.652190 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 3500: 1.641215 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 3600: 1.634380 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 3700: 1.631544 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 3800: 1.637764 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 3900: 1.614721 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 4000: 1.596207 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "================================================================================\n",
      " belommon one have retal ten of the proce most consuls her tatcher times longdom \n",
      "cultureasry empenes to program perlassios who the boh certains issue east implema\n",
      "rencell parth inaely in studoly one the pusuyear the battle actic conenducation r\n",
      "vk the which of the ct defister howards can the was facter it femtoep to the usea\n",
      "ew x god romagossing little in the remorge power are the lo safore see tead on th\n",
      "================================================================================\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 4100: 1.607434 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 7.43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 4200: 1.587065 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 7.64\n",
      "Average loss at step 4300: 1.593377 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 7.41\n",
      "Average loss at step 4400: 1.626986 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 7.19\n",
      "Average loss at step 4500: 1.634471 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 7.46\n",
      "Average loss at step 4600: 1.650345 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 4700: 1.635871 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 7.41\n",
      "Average loss at step 4800: 1.614855 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 6.82\n",
      "Average loss at step 4900: 1.613706 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 6.72\n",
      "Average loss at step 5000: 1.606267 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "================================================================================\n",
      "arast of originalls court of the killion deciderally adverically the eatvn remove\n",
      "kcianatedir famalling a scale robertion claristoriant was dring o so meads gs of \n",
      "pbull and and if the memperatimators keeski march gyl is pnce   in one nine five \n",
      "vks of greenway known unplearly one competer na two one nine zero outstolhhogreas\n",
      "bjectived with rescript to the ircularian serves in syllianamian two skard langua\n",
      "================================================================================\n",
      "Validation set perplexity: 6.94\n",
      "Average loss at step 5100: 1.585601 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 5200: 1.566439 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.22\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 5300: 1.585829 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 6.59\n",
      "Average loss at step 5400: 1.596526 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 6.63\n",
      "Average loss at step 5500: 1.597005 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 6.57\n",
      "Average loss at step 5600: 1.605923 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 6.61\n",
      "Average loss at step 5700: 1.581006 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 6.55\n",
      "Average loss at step 5800: 1.564375 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 6.56\n",
      "Average loss at step 5900: 1.575304 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 6.52\n",
      "Average loss at step 6000: 1.583443 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.45\n",
      "================================================================================\n",
      "ying or back which but completelies left ture instonia mination risk eight quite \n",
      "gdocts  monomy the ferenaster to be assually the chargettical inoritill b in the \n",
      "ipbu leen toke development critic wonly struments and europeans was departical cr\n",
      "mn joanization of the amid admines englished to be located padi christ changed on\n",
      "mgs blue acid primage oxisionally reconsidered acpartines ness in archited in the\n",
      "================================================================================\n",
      "Validation set perplexity: 6.59\n",
      "Average loss at step 6100: 1.582998 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 6.66\n",
      "Average loss at step 6200: 1.565715 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 6.56\n",
      "Average loss at step 6300: 1.582826 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 6.59\n",
      "Average loss at step 6400: 1.568418 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 6.53\n",
      "Average loss at step 6500: 1.562131 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 6.53\n",
      "Average loss at step 6600: 1.581420 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 6.61\n",
      "Average loss at step 6700: 1.588492 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 6.62\n",
      "Average loss at step 6800: 1.598564 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 6900: 1.577468 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 6.66\n",
      "Average loss at step 7000: 1.561195 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.22\n",
      "================================================================================\n",
      "hs laws letter history though two six poem kids trand provisis number transmy of \n",
      "mrs firhistory and infurtles only possible and jois of book progon of directions \n",
      "ces of edituve is peace as krone l sunmroaries weith is a senter uniana auguistai\n",
      "scients advared in medication discovin by the vy stoppons one nine three four twa\n",
      "cy set make hands to from simult of simply airs ba in heactarch who are which and\n",
      "================================================================================\n",
      "Validation set perplexity: 6.62\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[2:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "            bigram_feed = [sample(random_distribution()), \n",
    "                           sample(random_distribution())]\n",
    "            sentence = characters(bigram_feed[0])[0] + characters(bigram_feed[1])[0]\n",
    "            reset_sample_state.run()\n",
    "            for _ in range(79):\n",
    "                prediction = sample_prediction.eval({\n",
    "                  sample_input[0]: bigram_feed[-2],\n",
    "                  sample_input[1]: bigram_feed[-1]})\n",
    "                bigram_feed.append(sample(prediction))\n",
    "                sentence += characters(bigram_feed[-1])[0]\n",
    "            print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({\n",
    "                    sample_input[0]: b[0],\n",
    "                    sample_input[1]: b[1]\n",
    "            })\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c- Introduce Dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применение Dropout к RNN (LSTM) не давал положительных результатов. Zaremba, Sutskever, Vinyals в статье RECURRENT NEURAL NETWORK REGULARIZATION показали, что Dropout нужно применять к нерекуррентным связям, а к входам/выходам нейрона."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_dim = 128 # 10, 15, 32, 64, 100, 128 неплохо сработали\n",
    "num_nodes = 64\n",
    "drop_prob = 0.1\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  vocab_embed = tf.Variable(tf.random_uniform(\n",
    "                            [vocabulary_size * vocabulary_size, embed_dim], -1.0, 1.0))\n",
    "  \n",
    "  # Input gate, Forget gate, Memory cell, Output gate:\n",
    "  # input, previous output (or state), and bias.\n",
    "  wx = tf.Variable(tf.truncated_normal([embed_dim, 4 * num_nodes], -0.1, 0.1))\n",
    "  wm = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "  wb = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "  \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    # Получу матрицу со всеми необходимыми значениями, затем\n",
    "    # для каждого gate буду использовать части этой матрицы\n",
    "    gates_total = tf.matmul(i, wx) + tf.matmul(o, wm) + wb\n",
    "    \n",
    "    input_gate = tf.sigmoid(gates_total[:, :num_nodes])\n",
    "    forget_gate = tf.sigmoid(gates_total[:, num_nodes:2*num_nodes])\n",
    "    update = gates_total[:, 2*num_nodes:3*num_nodes]\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(gates_total[:, 3*num_nodes:])\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = []\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "  train_symbols = train_data[:num_unrollings]\n",
    "  # создаю zip-список биграмм вида \n",
    "  # [(м, ф), (ф, т), (т, и), (и, ' '), (' ', т), (т, о), (о, п)]\n",
    "  train_inputs = zip(train_symbols[:-1], train_symbols[1:])\n",
    "  train_labels = train_data[2:]  # сдвиг = 2, т.к. использую биграммы\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = []\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for train_bigram in train_inputs:\n",
    "    train_bigram_id = tf.argmax(train_bigram[0], dimension=1) + \\\n",
    "                vocabulary_size * tf.argmax(train_bigram[1], dimension=1)\n",
    "    train_bigram_embed = tf.nn.embedding_lookup(vocab_embed, train_bigram_id)\n",
    "    train_bigram_drop = tf.nn.dropout(train_bigram_embed, (1.0 - drop_prob))\n",
    "    output, state = lstm_cell(train_bigram_drop, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    logits_drop = tf.nn.dropout(logits, (1.0 - drop_prob))\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits_drop))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  #sample_uni_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  sample_input = [tf.placeholder(tf.float32, shape=[1, vocabulary_size]), \n",
    "                  tf.placeholder(tf.float32, shape=[1, vocabulary_size])]\n",
    "  sample_in_id = tf.argmax(sample_input[0], dimension=1) + vocabulary_size * tf.argmax(sample_input[1], dimension=1)\n",
    "  sample_in_embed = tf.nn.embedding_lookup(vocab_embed, sample_in_id)\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_in_embed, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/khansuleyman/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:170: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Average loss at step 0: 3.394319 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.82\n",
      "================================================================================\n",
      "ds tdoyeaoofsws d lghnnrk  jxarjsbgo xdavdg ohhmieueorhod oopyfe h  avite q sadco\n",
      "ojy so pqs pojkxreox qls kwtbhueemfhec tovl zfge o cir g lek qjlskqggyrraxqv neoq\n",
      "qq e fr vvngq festpk h ttbtskataejlbmh fnnlei acffch odgaz  cduweyrraledfsyff gsa\n",
      "gv beraafinvauasnrnoc k jn lctpw sn lec tbqbwdlsrveueeuuc kebooafwsuucri ct eg  i\n",
      "rj wnl p qe  ewybslqs o rs lj utslnekvqihi ra zte o feve qhg cljzu fowhhsetrgdrcz\n",
      "================================================================================\n",
      "Validation set perplexity: 19.48\n",
      "Average loss at step 100: 2.422416 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.83\n",
      "Validation set perplexity: 9.09\n",
      "Average loss at step 200: 2.136548 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.54\n",
      "Validation set perplexity: 8.22\n",
      "Average loss at step 300: 2.062691 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.65\n",
      "Validation set perplexity: 8.02\n",
      "Average loss at step 400: 2.026517 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.35\n",
      "Validation set perplexity: 7.63\n",
      "Average loss at step 500: 1.992856 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 7.84\n",
      "Average loss at step 600: 1.984038 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.97\n",
      "Validation set perplexity: 8.13\n",
      "Average loss at step 700: 1.964610 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 7.68\n",
      "Average loss at step 800: 1.926742 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 7.69\n",
      "Average loss at step 900: 1.899372 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 7.54\n",
      "Average loss at step 1000: 1.923158 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.66\n",
      "================================================================================\n",
      "qig interned is chree he five four four whenies in loepenserman reparqiect two fu\n",
      "ot thowe by a diffectituden known limes maytary write hyl were of lown the dessio\n",
      "um cented three compovliginal four five mindered organing neleast nembonally as w\n",
      "ix dufvid on imfrocs marvje the chaobrates powessary liicate kef requated from fl\n",
      "iwtters offerent woeft when h patmhole forcess xiergm the enderst memithe voy exp\n",
      "================================================================================\n",
      "Validation set perplexity: 7.38\n",
      "Average loss at step 1100: 1.883719 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 7.69\n",
      "Average loss at step 1200: 1.899069 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.11\n",
      "Validation set perplexity: 7.77\n",
      "Average loss at step 1300: 1.888051 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 1400: 1.876680 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 7.34\n",
      "Average loss at step 1500: 1.864124 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 7.53\n",
      "Average loss at step 1600: 1.860224 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 7.82\n",
      "Average loss at step 1700: 1.893401 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 7.48\n",
      "Average loss at step 1800: 1.870153 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 1900: 1.852416 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 2000: 1.864729 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "================================================================================\n",
      "os to has story as the phope accountrue de trussions tarien wons optized theo i m\n",
      "npuly of luvon s traission three zero fost than trolection of the and iral his to\n",
      "imprinedgan rone saits of leorpresideo geography numer that raica d frematmogy ne\n",
      "wlegissed zero zero four of the years s ascord studental pard pland most and conf\n",
      "ed and from suclobly relter blizacet of law the he elective there willaigy inmily\n",
      "================================================================================\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 2100: 1.863248 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 7.51\n",
      "Average loss at step 2200: 1.841059 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.67\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 2300: 1.863265 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 6.95\n",
      "Average loss at step 2400: 1.856902 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 2500: 1.837886 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 2600: 1.846433 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 2700: 1.873961 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 2800: 1.863627 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 7.04\n",
      "Average loss at step 2900: 1.905820 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 3000: 1.855375 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "================================================================================\n",
      "durant of yout squarity five two zero specife seed rivughhosred tbaks b to uklin \n",
      "wxulaents alyriord g glyrs out philosal one eight zero six had for senging with r\n",
      "ft substation one zero even have coverinauge the percent philogic kra pally durin\n",
      "les deen represion dakinging johnlements in eight two jackneakins amik able the p\n",
      "hartmcraspingn tef gefpd beads reputers man one one island methy lesds sence z th\n",
      "================================================================================\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 3100: 1.850735 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.00\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 3200: 1.855262 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 3300: 1.835723 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 3400: 1.842013 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 3500: 1.830747 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 7.34\n",
      "Average loss at step 3600: 1.852438 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 3700: 1.838213 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 7.15\n",
      "Average loss at step 3800: 1.839928 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 3900: 1.838823 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 4000: 1.836793 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "================================================================================\n",
      "gily surgal us chure appotlear in arts essass book move of claked one zero one ni\n",
      "bkast clintkstena that the threas roculirers in in vore garfield jenuff three th \n",
      " johnocuestium in asgrouran confirsm and more hans in for seventifer load nounts \n",
      "pticle bloicar also chinks and calvowans vore to lates areams ker a mgazes despoi\n",
      "jmnigh swearn stagnro aspacksors has first chankon boream million secutdtorsum we\n",
      "================================================================================\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 4100: 1.814556 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 7.21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 4200: 1.837448 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 4300: 1.842455 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 4400: 1.810337 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 4500: 1.843653 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 6.95\n",
      "Average loss at step 4600: 1.819819 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 4700: 1.823269 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 4800: 1.837183 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 6.95\n",
      "Average loss at step 4900: 1.824681 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.38\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 5000: 1.833552 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.37\n",
      "================================================================================\n",
      "fxle his injegld as union to ciament d of the three see egian al and teerr in the\n",
      "ts is toths father of irodan are his one ciplanity desrigion of propol of lesdy f\n",
      "mhddra lic himselte every was harral preside hele see his mab shlcuracticoarsm an\n",
      " unl li trounablight alexands paint given one issocektarly the trelbuid mary one \n",
      "sbr riatical te negr pcfescr ocnruent a stories reprean me telexie his polled pro\n",
      "================================================================================\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 5100: 1.809562 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 6.63\n",
      "Average loss at step 5200: 1.807673 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 6.60\n",
      "Average loss at step 5300: 1.812333 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 6.59\n",
      "Average loss at step 5400: 1.803454 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 6.52\n",
      "Average loss at step 5500: 1.781981 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 6.55\n",
      "Average loss at step 5600: 1.790657 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 6.53\n",
      "Average loss at step 5700: 1.807827 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 6.51\n",
      "Average loss at step 5800: 1.774126 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 6.48\n",
      "Average loss at step 5900: 1.769372 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 6000: 1.779388 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.58\n",
      "================================================================================\n",
      "wkical lays hysy is is gord in are the presena enxet in french huttens two commin\n",
      " jimman in used french of raphy namere fair whalking are s economitted to six zer\n",
      "v newspaches to when miculf are makembit of a revered to a grimate arbing iblousl\n",
      "sult for ormable of for tagon olded which her his a smagracisimdow larges made fi\n",
      "v japaded of the motheir the manx being single two olivelosprefferently spailinog\n",
      "================================================================================\n",
      "Validation set perplexity: 6.52\n",
      "Average loss at step 6100: 1.791204 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 6200: 1.790049 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 6.48\n",
      "Average loss at step 6300: 1.795808 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.38\n",
      "Validation set perplexity: 6.44\n",
      "Average loss at step 6400: 1.796801 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 6.47\n",
      "Average loss at step 6500: 1.810984 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 6600: 1.819549 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 6.45\n",
      "Average loss at step 6700: 1.810881 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 6800: 1.805199 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 6900: 1.784405 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 6.48\n",
      "Average loss at step 7000: 1.786909 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "================================================================================\n",
      "ylingeed of the togements voling if the confucite batther acress aloades bilitii \n",
      "su b one zero three zero nine seven nine zero eight three xer isdayi vems and gow\n",
      "mouthers acivoon w sect three of vis pez the woms bironies forcesic is ougory add\n",
      "zy cersital as more at degoney british enpingnks and by pimently the logion that \n",
      "rrenson efcomme seacrifs is roper end mensborm bisregimal seconds s lation chemar\n",
      "================================================================================\n",
      "Validation set perplexity: 6.46\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[2:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "            bigram_feed = [sample(random_distribution()), \n",
    "                           sample(random_distribution())]\n",
    "            sentence = characters(bigram_feed[0])[0] + characters(bigram_feed[1])[0]\n",
    "            reset_sample_state.run()\n",
    "            for _ in range(79):\n",
    "                prediction = sample_prediction.eval({\n",
    "                  sample_input[0]: bigram_feed[-2],\n",
    "                  sample_input[1]: bigram_feed[-1]})\n",
    "                bigram_feed.append(sample(prediction))\n",
    "                sentence += characters(bigram_feed[-1])[0]\n",
    "            print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({\n",
    "                    sample_input[0]: b[0],\n",
    "                    sample_input[1]: b[1]\n",
    "            })\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_text_split = train_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_text_split_reverse = [word[::-1] for word in train_text_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gatherer', 'bands', 'were', 'egalitarian', 'and', 'lacked', 'division', 'of', 'labour', 'accumulated']\n",
      "['rerehtag', 'sdnab', 'erew', 'nairatilage', 'dna', 'dekcal', 'noisivid', 'fo', 'ruobal', 'detalumucca']\n"
     ]
    }
   ],
   "source": [
    "print(train_text_split[100:110])\n",
    "print(train_text_split_reverse[100:110])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/text8reversed.txt', 'w') as f:\n",
    "  f.write('\\n'.join(train_text_split_reverse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/text8original.txt', 'w') as f:\n",
    "  f.write('\\n'.join(train_text_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_text_split_sentences = []\n",
    "train_text_split_reversed_sentences = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_sentence = []\n",
    "rev_sentence = []\n",
    "for i in range(len(train_text_split)):\n",
    "  if i % 5 == 0:\n",
    "    train_text_split_sentences.append(' '.join(orig_sentence))\n",
    "    train_text_split_reversed_sentences.append(' '.join(rev_sentence))\n",
    "    orig_sentence = []\n",
    "    rev_sentence = []\n",
    "  else:\n",
    "    orig_sentence.append(train_text_split[i])\n",
    "    rev_sentence.append(train_text_split_reverse[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'stsihcrana etacovda laicos snoitaler', 'nopu yratnulov noitaicossa fo', 'slaudividni lautum dia dna', 'ecnanrevog elihw msihcrana si']\n",
      "['', 'anarchists advocate social relations', 'upon voluntary association of', 'individuals mutual aid and', 'governance while anarchism is']\n"
     ]
    }
   ],
   "source": [
    "print(train_text_split_reversed_sentences[:5])\n",
    "print(train_text_split_sentences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/text8_sentences_original.txt', 'w') as f:\n",
    "  f.write('\\n'.join(train_text_split_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/text8_sentences_reversed.txt', 'w') as f:\n",
    "  f.write('\\n'.join(train_text_split_reversed_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Отражает только слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import helper\n",
    "\n",
    "source_path = 'data/text8original.txt'\n",
    "target_path = 'data/text8reversed.txt'\n",
    "#source_path = 'data/text8_sentences_original.txt'\n",
    "#target_path = 'data/text8_sentences_reversed.txt'\n",
    "\n",
    "source_sentences = helper.load_data(source_path)\n",
    "target_sentences = helper.load_data(target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ons', 'anarchists', 'advocate', 'social', 'relations', 'based', 'upo']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_sentences[:50].split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sno', 'stsihcrana', 'etacovda', 'laicos', 'snoitaler', 'desab', 'nop']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_sentences[:50].split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example source sequence\n",
      "[[14, 28, 19], [6, 28, 6, 23, 12, 22, 25, 19, 20, 19], [6, 18, 4, 14, 12, 6, 20, 26]]\n",
      "\n",
      "\n",
      "Example target sequence\n",
      "[[19, 28, 14, 3], [19, 20, 19, 25, 22, 12, 23, 6, 28, 6, 3], [26, 20, 6, 12, 14, 4, 18, 6, 3]]\n"
     ]
    }
   ],
   "source": [
    "def extract_character_vocab(data):\n",
    "    special_words = ['<PAD>', '<UNK>', '<GO>',  '<EOS>']\n",
    "\n",
    "    set_words = set([character for line in data.split('\\n') for character in line])\n",
    "    int_to_vocab = {word_i: word for word_i, word in enumerate(special_words + list(set_words))}\n",
    "    vocab_to_int = {word: word_i for word_i, word in int_to_vocab.items()}\n",
    "\n",
    "    return int_to_vocab, vocab_to_int\n",
    "\n",
    "source_int_to_letter, source_letter_to_int = extract_character_vocab(source_sentences)\n",
    "target_int_to_letter, target_letter_to_int = extract_character_vocab(target_sentences)\n",
    "\n",
    "source_letter_ids = [[source_letter_to_int.get(letter, source_letter_to_int['<UNK>']) for letter in line] for line in source_sentences.split('\\n')]\n",
    "target_letter_ids = [[target_letter_to_int.get(letter, target_letter_to_int['<UNK>']) for letter in line] + [target_letter_to_int['<EOS>']] for line in target_sentences.split('\\n')] \n",
    "\n",
    "print(\"Example source sequence\")\n",
    "print(source_letter_ids[:3])\n",
    "print(\"\\n\")\n",
    "print(\"Example target sequence\")\n",
    "print(target_letter_ids[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "batch_size = 128\n",
    "rnn_size = 50\n",
    "num_layers = 2\n",
    "encoding_embedding_size = 15\n",
    "decoding_embedding_size = 15\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_inputs():\n",
    "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "\n",
    "    target_sequence_length = tf.placeholder(tf.int32, (None,), name='target_sequence_length')\n",
    "    max_target_sequence_length = tf.reduce_max(target_sequence_length, name='max_target_len')\n",
    "    source_sequence_length = tf.placeholder(tf.int32, (None,), name='source_sequence_length')\n",
    "    \n",
    "    return input_data, targets, lr, target_sequence_length, max_target_sequence_length, source_sequence_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encoding_layer(input_data, rnn_size, num_layers,\n",
    "                   source_sequence_length, source_vocab_size, \n",
    "                   encoding_embedding_size):\n",
    "\n",
    "    enc_embed_input = tf.contrib.layers.embed_sequence(input_data, source_vocab_size, encoding_embedding_size)\n",
    "\n",
    "    def make_cell(rnn_size):\n",
    "        enc_cell = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                           initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "        return enc_cell\n",
    "\n",
    "    enc_cell = tf.contrib.rnn.MultiRNNCell([make_cell(rnn_size) for _ in range(num_layers)])\n",
    "    \n",
    "    enc_output, enc_state = tf.nn.dynamic_rnn(enc_cell, enc_embed_input, sequence_length=source_sequence_length, dtype=tf.float32)\n",
    "    \n",
    "    return enc_output, enc_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_decoder_input(target_data, vocab_to_int, batch_size):\n",
    "    '''Remove the last word id from each batch and concat the <GO> to the begining of each batch'''\n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
    "\n",
    "    return dec_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decoding_layer(target_letter_to_int, decoding_embedding_size, num_layers, rnn_size,\n",
    "                   target_sequence_length, max_target_sequence_length, enc_state, dec_input):\n",
    "\n",
    "    target_vocab_size = len(target_letter_to_int)\n",
    "    dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size]))\n",
    "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
    "\n",
    "    def make_cell(rnn_size):\n",
    "        dec_cell = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                           initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "        return dec_cell\n",
    "\n",
    "    dec_cell = tf.contrib.rnn.MultiRNNCell([make_cell(rnn_size) for _ in range(num_layers)])\n",
    "     \n",
    "    output_layer = Dense(target_vocab_size,\n",
    "                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
    "\n",
    "    with tf.variable_scope(\"decode\"):\n",
    "\n",
    "        training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
    "                                                            sequence_length=target_sequence_length,\n",
    "                                                            time_major=False)\n",
    "        \n",
    "        \n",
    "        training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                           training_helper,\n",
    "                                                           enc_state,\n",
    "                                                           output_layer) \n",
    "        \n",
    "        training_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "                                                                       impute_finished=True,\n",
    "                                                                       maximum_iterations=max_target_sequence_length)\n",
    "    \n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "        start_tokens = tf.tile(tf.constant([target_letter_to_int['<GO>']], dtype=tf.int32), [batch_size], name='start_tokens')\n",
    "\n",
    "        inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(dec_embeddings,\n",
    "                                                                start_tokens,\n",
    "                                                                target_letter_to_int['<EOS>'])\n",
    "\n",
    "        inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                        inference_helper,\n",
    "                                                        enc_state,\n",
    "                                                        output_layer)\n",
    "        \n",
    "        inference_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
    "                                                            impute_finished=True,\n",
    "                                                            maximum_iterations=max_target_sequence_length)\n",
    "         \n",
    "\n",
    "    \n",
    "    return training_decoder_output, inference_decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def seq2seq_model(input_data, targets, lr, target_sequence_length, \n",
    "                  max_target_sequence_length, source_sequence_length,\n",
    "                  source_vocab_size, target_vocab_size,\n",
    "                  enc_embedding_size, dec_embedding_size, \n",
    "                  rnn_size, num_layers):\n",
    "    \n",
    "    _, enc_state = encoding_layer(input_data, \n",
    "                                  rnn_size, \n",
    "                                  num_layers, \n",
    "                                  source_sequence_length,\n",
    "                                  source_vocab_size, \n",
    "                                  encoding_embedding_size)\n",
    "    \n",
    "    \n",
    "    dec_input = process_decoder_input(targets, target_letter_to_int, batch_size)\n",
    "    \n",
    "    training_decoder_output, inference_decoder_output = decoding_layer(target_letter_to_int, \n",
    "                                                                       decoding_embedding_size, \n",
    "                                                                       num_layers, \n",
    "                                                                       rnn_size,\n",
    "                                                                       target_sequence_length,\n",
    "                                                                       max_target_sequence_length,\n",
    "                                                                       enc_state, \n",
    "                                                                       dec_input) \n",
    "    \n",
    "    return training_decoder_output, inference_decoder_output\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    input_data, targets, lr, target_sequence_length, max_target_sequence_length, source_sequence_length = get_model_inputs()\n",
    "    \n",
    "    training_decoder_output, inference_decoder_output = seq2seq_model(input_data, \n",
    "                                                                      targets, \n",
    "                                                                      lr, \n",
    "                                                                      target_sequence_length, \n",
    "                                                                      max_target_sequence_length, \n",
    "                                                                      source_sequence_length,\n",
    "                                                                      len(source_letter_to_int),\n",
    "                                                                      len(target_letter_to_int),\n",
    "                                                                      encoding_embedding_size, \n",
    "                                                                      decoding_embedding_size, \n",
    "                                                                      rnn_size, \n",
    "                                                                      num_layers)    \n",
    "    \n",
    "    training_logits = tf.identity(training_decoder_output.rnn_output, 'logits')\n",
    "    inference_logits = tf.identity(inference_decoder_output.sample_id, name='predictions')\n",
    "    \n",
    "    masks = tf.sequence_mask(target_sequence_length, max_target_sequence_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        \n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            training_logits,\n",
    "            targets,\n",
    "            masks)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch, pad_int):\n",
    "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [pad_int] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(targets, sources, batch_size, source_pad_int, target_pad_int):\n",
    "    \"\"\"Batch targets, sources, and the lengths of their sentences together\"\"\"\n",
    "    for batch_i in range(0, len(sources)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        sources_batch = sources[start_i:start_i + batch_size]\n",
    "        targets_batch = targets[start_i:start_i + batch_size]\n",
    "        pad_sources_batch = np.array(pad_sentence_batch(sources_batch, source_pad_int))\n",
    "        pad_targets_batch = np.array(pad_sentence_batch(targets_batch, target_pad_int))\n",
    "        \n",
    "        pad_targets_lengths = []\n",
    "        for target in pad_targets_batch:\n",
    "            pad_targets_lengths.append(len(target))\n",
    "        \n",
    "        pad_source_lengths = []\n",
    "        for source in pad_sources_batch:\n",
    "            pad_source_lengths.append(len(source))\n",
    "        \n",
    "        yield pad_targets_batch, pad_sources_batch, pad_targets_lengths, pad_source_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/5 Batch 1000/132850 - Loss:  0.812  - Validation loss:  0.882\n",
      "Epoch   1/5 Batch 2000/132850 - Loss:  0.588  - Validation loss:  0.721\n",
      "Epoch   1/5 Batch 3000/132850 - Loss:  0.328  - Validation loss:  0.520\n",
      "Epoch   1/5 Batch 4000/132850 - Loss:  0.278  - Validation loss:  0.334\n",
      "Epoch   1/5 Batch 5000/132850 - Loss:  0.177  - Validation loss:  0.220\n",
      "Epoch   1/5 Batch 6000/132850 - Loss:  0.083  - Validation loss:  0.140\n",
      "Epoch   1/5 Batch 7000/132850 - Loss:  0.070  - Validation loss:  0.088\n",
      "Epoch   1/5 Batch 8000/132850 - Loss:  0.060  - Validation loss:  0.067\n",
      "Epoch   1/5 Batch 9000/132850 - Loss:  0.061  - Validation loss:  0.040\n",
      "Epoch   1/5 Batch 10000/132850 - Loss:  0.017  - Validation loss:  0.037\n",
      "Epoch   1/5 Batch 11000/132850 - Loss:  0.041  - Validation loss:  0.032\n",
      "Epoch   1/5 Batch 12000/132850 - Loss:  0.016  - Validation loss:  0.032\n",
      "Epoch   1/5 Batch 13000/132850 - Loss:  0.012  - Validation loss:  0.017\n",
      "Epoch   1/5 Batch 14000/132850 - Loss:  0.018  - Validation loss:  0.014\n",
      "Epoch   1/5 Batch 15000/132850 - Loss:  0.006  - Validation loss:  0.011\n",
      "Epoch   1/5 Batch 16000/132850 - Loss:  0.015  - Validation loss:  0.028\n",
      "Epoch   1/5 Batch 17000/132850 - Loss:  0.007  - Validation loss:  0.008\n",
      "Epoch   1/5 Batch 18000/132850 - Loss:  0.003  - Validation loss:  0.009\n",
      "Epoch   1/5 Batch 19000/132850 - Loss:  0.004  - Validation loss:  0.005\n",
      "Epoch   1/5 Batch 20000/132850 - Loss:  0.007  - Validation loss:  0.027\n",
      "Epoch   1/5 Batch 21000/132850 - Loss:  0.012  - Validation loss:  0.010\n",
      "Epoch   1/5 Batch 22000/132850 - Loss:  0.002  - Validation loss:  0.005\n",
      "Epoch   1/5 Batch 23000/132850 - Loss:  0.000  - Validation loss:  0.003\n",
      "Epoch   1/5 Batch 24000/132850 - Loss:  0.002  - Validation loss:  0.002\n",
      "Epoch   1/5 Batch 25000/132850 - Loss:  0.001  - Validation loss:  0.005\n",
      "Epoch   1/5 Batch 26000/132850 - Loss:  0.000  - Validation loss:  0.007\n",
      "Epoch   1/5 Batch 27000/132850 - Loss:  0.001  - Validation loss:  0.002\n",
      "Epoch   1/5 Batch 28000/132850 - Loss:  0.000  - Validation loss:  0.003\n",
      "Epoch   1/5 Batch 29000/132850 - Loss:  0.001  - Validation loss:  0.005\n",
      "Epoch   1/5 Batch 30000/132850 - Loss:  0.001  - Validation loss:  0.002\n",
      "Epoch   1/5 Batch 31000/132850 - Loss:  0.001  - Validation loss:  0.004\n",
      "Epoch   1/5 Batch 32000/132850 - Loss:  0.001  - Validation loss:  0.001\n",
      "Epoch   1/5 Batch 33000/132850 - Loss:  0.003  - Validation loss:  0.002\n",
      "Epoch   1/5 Batch 34000/132850 - Loss:  0.001  - Validation loss:  0.009\n",
      "Epoch   1/5 Batch 35000/132850 - Loss:  0.002  - Validation loss:  0.003\n",
      "Epoch   1/5 Batch 36000/132850 - Loss:  0.000  - Validation loss:  0.001\n",
      "Epoch   1/5 Batch 37000/132850 - Loss:  0.001  - Validation loss:  0.007\n",
      "Epoch   1/5 Batch 38000/132850 - Loss:  0.001  - Validation loss:  0.001\n",
      "Epoch   1/5 Batch 39000/132850 - Loss:  0.001  - Validation loss:  0.001\n",
      "Epoch   1/5 Batch 40000/132850 - Loss:  0.000  - Validation loss:  0.001\n",
      "Epoch   1/5 Batch 41000/132850 - Loss:  0.005  - Validation loss:  0.005\n",
      "Epoch   1/5 Batch 42000/132850 - Loss:  0.000  - Validation loss:  0.004\n",
      "Epoch   1/5 Batch 43000/132850 - Loss:  0.001  - Validation loss:  0.002\n",
      "Epoch   1/5 Batch 44000/132850 - Loss:  0.001  - Validation loss:  0.001\n",
      "Epoch   1/5 Batch 45000/132850 - Loss:  0.001  - Validation loss:  0.001\n",
      "Epoch   1/5 Batch 46000/132850 - Loss:  0.000  - Validation loss:  0.001\n",
      "Epoch   1/5 Batch 47000/132850 - Loss:  0.000  - Validation loss:  0.001\n",
      "Epoch   1/5 Batch 48000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   1/5 Batch 49000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   1/5 Batch 50000/132850 - Loss:  0.001  - Validation loss:  0.003\n",
      "Epoch   1/5 Batch 51000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   1/5 Batch 52000/132850 - Loss:  0.000  - Validation loss:  0.001\n",
      "Epoch   1/5 Batch 53000/132850 - Loss:  0.000  - Validation loss:  0.001\n",
      "Epoch   1/5 Batch 54000/132850 - Loss:  0.000  - Validation loss:  0.003\n",
      "Epoch   1/5 Batch 55000/132850 - Loss:  0.000  - Validation loss:  0.001\n",
      "Epoch   1/5 Batch 56000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   1/5 Batch 57000/132850 - Loss:  0.008  - Validation loss:  0.005\n",
      "Epoch   1/5 Batch 58000/132850 - Loss:  0.000  - Validation loss:  0.001\n",
      "Epoch   1/5 Batch 59000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   1/5 Batch 60000/132850 - Loss:  0.003  - Validation loss:  0.001\n",
      "Epoch   1/5 Batch 61000/132850 - Loss:  0.000  - Validation loss:  0.001\n",
      "Epoch   1/5 Batch 62000/132850 - Loss:  0.000  - Validation loss:  0.001\n",
      "Epoch   1/5 Batch 63000/132850 - Loss:  0.001  - Validation loss:  0.001\n",
      "Epoch   1/5 Batch 64000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   1/5 Batch 65000/132850 - Loss:  0.000  - Validation loss:  0.001\n",
      "Epoch   1/5 Batch 66000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   1/5 Batch 67000/132850 - Loss:  0.001  - Validation loss:  0.002\n",
      "Epoch   1/5 Batch 68000/132850 - Loss:  0.001  - Validation loss:  0.003\n",
      "Epoch   1/5 Batch 69000/132850 - Loss:  0.000  - Validation loss:  0.001\n",
      "Epoch   1/5 Batch 70000/132850 - Loss:  0.002  - Validation loss:  0.001\n",
      "Epoch   1/5 Batch 71000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   1/5 Batch 72000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   1/5 Batch 73000/132850 - Loss:  0.001  - Validation loss:  0.002\n",
      "Epoch   1/5 Batch 74000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   1/5 Batch 75000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   1/5 Batch 76000/132850 - Loss:  0.000  - Validation loss:  0.001\n",
      "Epoch   1/5 Batch 77000/132850 - Loss:  0.001  - Validation loss:  0.001\n",
      "Epoch   1/5 Batch 78000/132850 - Loss:  0.001  - Validation loss:  0.001\n",
      "Epoch   1/5 Batch 79000/132850 - Loss:  0.001  - Validation loss:  0.000\n",
      "Epoch   1/5 Batch 80000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   1/5 Batch 81000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   1/5 Batch 82000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   1/5 Batch 83000/132850 - Loss:  0.000  - Validation loss:  0.001\n",
      "Epoch   1/5 Batch 84000/132850 - Loss:  0.000  - Validation loss:  0.001\n",
      "Epoch   1/5 Batch 85000/132850 - Loss:  0.001  - Validation loss:  0.004\n",
      "Epoch   1/5 Batch 86000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   1/5 Batch 87000/132850 - Loss:  0.001  - Validation loss:  0.000\n",
      "Epoch   1/5 Batch 88000/132850 - Loss:  0.000  - Validation loss:  0.001\n",
      "Epoch   1/5 Batch 89000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   1/5 Batch 90000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   1/5 Batch 91000/132850 - Loss:  0.002  - Validation loss:  0.000\n",
      "Epoch   1/5 Batch 92000/132850 - Loss:  0.004  - Validation loss:  0.001\n",
      "Epoch   1/5 Batch 93000/132850 - Loss:  0.000  - Validation loss:  0.003\n",
      "Epoch   1/5 Batch 94000/132850 - Loss:  0.000  - Validation loss:  0.001\n",
      "Epoch   1/5 Batch 95000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   1/5 Batch 96000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   1/5 Batch 97000/132850 - Loss:  0.000  - Validation loss:  0.001\n",
      "Epoch   1/5 Batch 98000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   1/5 Batch 99000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   1/5 Batch 100000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   1/5 Batch 101000/132850 - Loss:  0.001  - Validation loss:  0.000\n",
      "Epoch   1/5 Batch 102000/132850 - Loss:  0.000  - Validation loss:  0.001\n",
      "Epoch   1/5 Batch 103000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   1/5 Batch 104000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   1/5 Batch 105000/132850 - Loss:  0.000  - Validation loss:  0.001\n",
      "Epoch   1/5 Batch 106000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   1/5 Batch 107000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   1/5 Batch 108000/132850 - Loss:  0.000  - Validation loss:  0.003\n",
      "Epoch   1/5 Batch 109000/132850 - Loss:  0.001  - Validation loss:  0.001\n",
      "Epoch   1/5 Batch 110000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   1/5 Batch 111000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   1/5 Batch 112000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   1/5 Batch 113000/132850 - Loss:  0.000  - Validation loss:  0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/5 Batch 114000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   1/5 Batch 115000/132850 - Loss:  0.000  - Validation loss:  0.001\n",
      "Epoch   1/5 Batch 116000/132850 - Loss:  0.000  - Validation loss:  0.001\n",
      "Epoch   1/5 Batch 117000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   1/5 Batch 118000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   1/5 Batch 119000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   1/5 Batch 120000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   1/5 Batch 121000/132850 - Loss:  0.007  - Validation loss:  0.001\n",
      "Epoch   1/5 Batch 122000/132850 - Loss:  0.001  - Validation loss:  0.000\n",
      "Epoch   1/5 Batch 123000/132850 - Loss:  0.000  - Validation loss:  0.002\n",
      "Epoch   1/5 Batch 124000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   1/5 Batch 125000/132850 - Loss:  0.001  - Validation loss:  0.002\n",
      "Epoch   1/5 Batch 126000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   1/5 Batch 127000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   1/5 Batch 128000/132850 - Loss:  0.001  - Validation loss:  0.003\n",
      "Epoch   1/5 Batch 129000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   1/5 Batch 130000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   1/5 Batch 131000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   1/5 Batch 132000/132850 - Loss:  0.000  - Validation loss:  0.003\n",
      "Epoch   2/5 Batch 1000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 2000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 3000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 4000/132850 - Loss:  0.000  - Validation loss:  0.002\n",
      "Epoch   2/5 Batch 5000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 6000/132850 - Loss:  0.000  - Validation loss:  0.001\n",
      "Epoch   2/5 Batch 7000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 8000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 9000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 10000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 11000/132850 - Loss:  0.004  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 12000/132850 - Loss:  0.000  - Validation loss:  0.001\n",
      "Epoch   2/5 Batch 13000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 14000/132850 - Loss:  0.001  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 15000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 16000/132850 - Loss:  0.000  - Validation loss:  0.001\n",
      "Epoch   2/5 Batch 17000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 18000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 19000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 20000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 21000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 22000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 23000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 24000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 25000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 26000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 27000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 28000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 29000/132850 - Loss:  0.002  - Validation loss:  0.003\n",
      "Epoch   2/5 Batch 30000/132850 - Loss:  0.000  - Validation loss:  0.001\n",
      "Epoch   2/5 Batch 31000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 32000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 33000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 34000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 35000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 36000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 37000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 38000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 39000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 40000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 41000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 42000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 43000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 44000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 45000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 46000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 47000/132850 - Loss:  0.000  - Validation loss:  0.002\n",
      "Epoch   2/5 Batch 48000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 49000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 50000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 51000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 52000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 53000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 54000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 55000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 56000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 57000/132850 - Loss:  0.006  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 58000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 59000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 60000/132850 - Loss:  0.001  - Validation loss:  0.001\n",
      "Epoch   2/5 Batch 61000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 62000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 63000/132850 - Loss:  0.000  - Validation loss:  0.004\n",
      "Epoch   2/5 Batch 64000/132850 - Loss:  0.000  - Validation loss:  0.002\n",
      "Epoch   2/5 Batch 65000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 66000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 67000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 68000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 69000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 70000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 71000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 72000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 73000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 74000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 75000/132850 - Loss:  0.000  - Validation loss:  0.001\n",
      "Epoch   2/5 Batch 76000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 77000/132850 - Loss:  0.000  - Validation loss:  0.001\n",
      "Epoch   2/5 Batch 78000/132850 - Loss:  0.000  - Validation loss:  0.001\n",
      "Epoch   2/5 Batch 79000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 80000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 81000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 82000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 83000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 84000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 85000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 86000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 87000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 88000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 89000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 90000/132850 - Loss:  0.001  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 91000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 92000/132850 - Loss:  0.002  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 93000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 94000/132850 - Loss:  0.003  - Validation loss:  0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2/5 Batch 95000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 96000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 97000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 98000/132850 - Loss:  0.000  - Validation loss:  0.001\n",
      "Epoch   2/5 Batch 99000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 100000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 101000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 102000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 103000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 104000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 105000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 106000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 107000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 108000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 109000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 110000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 111000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 112000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 113000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 114000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 115000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 116000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 117000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 118000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 119000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 120000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 121000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 122000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 123000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 124000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 125000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 126000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 127000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 128000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 129000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 130000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   2/5 Batch 131000/132850 - Loss:  0.000  - Validation loss:  0.006\n",
      "Epoch   2/5 Batch 132000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 1000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 2000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 3000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 4000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 5000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 6000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 7000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 8000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 9000/132850 - Loss:  0.001  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 10000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 11000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 12000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 13000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 14000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 15000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 16000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 17000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 18000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 19000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 20000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 21000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 22000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 23000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 24000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 25000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 26000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 27000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 28000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 29000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 30000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 31000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 32000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 33000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 34000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 35000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 36000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 37000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 38000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 39000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 40000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 41000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 42000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 43000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 44000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 45000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 46000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 47000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 48000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 49000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 50000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 51000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 52000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 53000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 54000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 55000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 56000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 57000/132850 - Loss:  0.004  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 58000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 59000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 60000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 61000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 62000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 63000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 64000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 65000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 66000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 67000/132850 - Loss:  0.000  - Validation loss:  0.001\n",
      "Epoch   3/5 Batch 68000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 69000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 70000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 71000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 72000/132850 - Loss:  0.000  - Validation loss:  0.002\n",
      "Epoch   3/5 Batch 73000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 74000/132850 - Loss:  0.000  - Validation loss:  0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   3/5 Batch 75000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 76000/132850 - Loss:  0.000  - Validation loss:  0.001\n",
      "Epoch   3/5 Batch 77000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 78000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 79000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 80000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 81000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 82000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 83000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 84000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 85000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 86000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 87000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 88000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 89000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 90000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 91000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 92000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 93000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 94000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 95000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 96000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 97000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 98000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 99000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 100000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 101000/132850 - Loss:  0.000  - Validation loss:  0.001\n",
      "Epoch   3/5 Batch 102000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 103000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 104000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 105000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 106000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 107000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 108000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 109000/132850 - Loss:  0.001  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 110000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 111000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 112000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 113000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 114000/132850 - Loss:  0.000  - Validation loss:  0.001\n",
      "Epoch   3/5 Batch 115000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 116000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 117000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 118000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 119000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 120000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 121000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 122000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 123000/132850 - Loss:  0.000  - Validation loss:  0.003\n",
      "Epoch   3/5 Batch 124000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 125000/132850 - Loss:  0.002  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 126000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 127000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 128000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 129000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 130000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 131000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   3/5 Batch 132000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 1000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 2000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 3000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 4000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 5000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 6000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 7000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 8000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 9000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 10000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 11000/132850 - Loss:  0.001  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 12000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 13000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 14000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 15000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 16000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 17000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 18000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 19000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 20000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 21000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 22000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 23000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 24000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 25000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 26000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 27000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 28000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 29000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 30000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 31000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 32000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 33000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 34000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 35000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 36000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 37000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 38000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 39000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 40000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 41000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 42000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 43000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 44000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 45000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 46000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 47000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 48000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 49000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 50000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 51000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 52000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 53000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 54000/132850 - Loss:  0.000  - Validation loss:  0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   4/5 Batch 55000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 56000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 57000/132850 - Loss:  0.006  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 58000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 59000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 60000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 61000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 62000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 63000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 64000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 65000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 66000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 67000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 68000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 69000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 70000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 71000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 72000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 73000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 74000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 75000/132850 - Loss:  0.001  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 76000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 77000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 78000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 79000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 80000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 81000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 82000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 83000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 84000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 85000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 86000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 87000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 88000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 89000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 90000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 91000/132850 - Loss:  0.001  - Validation loss:  0.001\n",
      "Epoch   4/5 Batch 92000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 93000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 94000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 95000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 96000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 97000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 98000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 99000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 100000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 101000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 102000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 103000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 104000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 105000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 106000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 107000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 108000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 109000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 110000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 111000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 112000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 113000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 114000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 115000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 116000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 117000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 118000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 119000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 120000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 121000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 122000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 123000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 124000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 125000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 126000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 127000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 128000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 129000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 130000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 131000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   4/5 Batch 132000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 1000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 2000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 3000/132850 - Loss:  0.000  - Validation loss:  0.003\n",
      "Epoch   5/5 Batch 4000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 5000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 6000/132850 - Loss:  0.001  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 7000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 8000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 9000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 10000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 11000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 12000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 13000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 14000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 15000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 16000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 17000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 18000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 19000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 20000/132850 - Loss:  0.000  - Validation loss:  0.001\n",
      "Epoch   5/5 Batch 21000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 22000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 23000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 24000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 25000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 26000/132850 - Loss:  0.000  - Validation loss:  0.001\n",
      "Epoch   5/5 Batch 27000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 28000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 29000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 30000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 31000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 32000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 33000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 34000/132850 - Loss:  0.000  - Validation loss:  0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   5/5 Batch 35000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 36000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 37000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 38000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 39000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 40000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 41000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 42000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 43000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 44000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 45000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 46000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 47000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 48000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 49000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 50000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 51000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 52000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 53000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 54000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 55000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 56000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 57000/132850 - Loss:  0.002  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 58000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 59000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 60000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 61000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 62000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 63000/132850 - Loss:  0.000  - Validation loss:  0.001\n",
      "Epoch   5/5 Batch 64000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 65000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 66000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 67000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 68000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 69000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 70000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 71000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 72000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 73000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 74000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 75000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 76000/132850 - Loss:  0.001  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 77000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 78000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 79000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 80000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 81000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 82000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 83000/132850 - Loss:  0.000  - Validation loss:  0.001\n",
      "Epoch   5/5 Batch 84000/132850 - Loss:  0.000  - Validation loss:  0.001\n",
      "Epoch   5/5 Batch 85000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 86000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 87000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 88000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 89000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 90000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 91000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 92000/132850 - Loss:  0.000  - Validation loss:  0.001\n",
      "Epoch   5/5 Batch 93000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 94000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 95000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 96000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 97000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 98000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 99000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 100000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 101000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 102000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 103000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 104000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 105000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 106000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 107000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 108000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 109000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 110000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 111000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 112000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 113000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 114000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 115000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 116000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 117000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 118000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 119000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 120000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 121000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 122000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 123000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 124000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 125000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 126000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 127000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 128000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 129000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 130000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 131000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Epoch   5/5 Batch 132000/132850 - Loss:  0.000  - Validation loss:  0.000\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "train_source = source_letter_ids[batch_size:]\n",
    "train_target = target_letter_ids[batch_size:]\n",
    "valid_source = source_letter_ids[:batch_size]\n",
    "valid_target = target_letter_ids[:batch_size]\n",
    "(valid_targets_batch, valid_sources_batch, valid_targets_lengths, valid_sources_lengths) = next(get_batches(valid_target, valid_source, batch_size,\n",
    "                           source_letter_to_int['<PAD>'],\n",
    "                           target_letter_to_int['<PAD>']))\n",
    "\n",
    "display_step = 1000\n",
    "\n",
    "checkpoint = \"best_model.ckpt\" \n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    for epoch_i in range(1, epochs+1):\n",
    "        for batch_i, (targets_batch, sources_batch, targets_lengths, sources_lengths) in enumerate(\n",
    "                get_batches(train_target, train_source, batch_size,\n",
    "                           source_letter_to_int['<PAD>'],\n",
    "                           target_letter_to_int['<PAD>'])):\n",
    "            \n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: sources_batch,\n",
    "                 targets: targets_batch,\n",
    "                 lr: learning_rate,\n",
    "                 target_sequence_length: targets_lengths,\n",
    "                 source_sequence_length: sources_lengths})\n",
    "\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "                \n",
    "                validation_loss = sess.run(\n",
    "                [cost],\n",
    "                {input_data: valid_sources_batch,\n",
    "                 targets: valid_targets_batch,\n",
    "                 lr: learning_rate,\n",
    "                 target_sequence_length: valid_targets_lengths,\n",
    "                 source_sequence_length: valid_sources_lengths})\n",
    "                \n",
    "                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}  - Validation loss: {:>6.3f}'\n",
    "                      .format(epoch_i,\n",
    "                              epochs, \n",
    "                              batch_i, \n",
    "                              len(train_source) // batch_size, \n",
    "                              loss, \n",
    "                              validation_loss[0]))\n",
    "\n",
    "    \n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, checkpoint)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def source_to_seq(text):\n",
    "    '''Prepare the text for the model'''\n",
    "    sequence_length = 9\n",
    "    return [source_letter_to_int.get(word, source_letter_to_int['<UNK>']) for word in text]+ [source_letter_to_int['<PAD>']]*(sequence_length-len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./best_model.ckpt\n",
      "Original Text: jjjgjjgj\n",
      "\n",
      "Source\n",
      "  Word Ids:    [16, 16, 16, 14, 16, 16, 14, 16, 0]\n",
      "  Input Words: j j j g j j g j <PAD>\n",
      "\n",
      "Target\n",
      "  Word Ids:       [16, 14, 16, 16, 14, 16, 16, 16, 3]\n",
      "  Response Words: j g j j g j j j <EOS>\n"
     ]
    }
   ],
   "source": [
    "input_sentence = 'jjjgjjgj'\n",
    "text = source_to_seq(input_sentence)\n",
    "\n",
    "checkpoint = \"./best_model.ckpt\"\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "    loader.restore(sess, checkpoint)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    source_sequence_length = loaded_graph.get_tensor_by_name('source_sequence_length:0')\n",
    "    target_sequence_length = loaded_graph.get_tensor_by_name('target_sequence_length:0')\n",
    "    \n",
    "    #Multiply by batch_size to match the model's input parameters\n",
    "    answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
    "                                      target_sequence_length: [len(text)]*batch_size, \n",
    "                                      source_sequence_length: [len(text)]*batch_size})[0] \n",
    "\n",
    "\n",
    "pad = source_letter_to_int[\"<PAD>\"] \n",
    "\n",
    "print('Original Text:', input_sentence)\n",
    "\n",
    "print('\\nSource')\n",
    "print('  Word Ids:    {}'.format([i for i in text]))\n",
    "print('  Input Words: {}'.format(\" \".join([source_int_to_letter[i] for i in text])))\n",
    "\n",
    "print('\\nTarget')\n",
    "print('  Word Ids:       {}'.format([i for i in answer_logits if i != pad]))\n",
    "print('  Response Words: {}'.format(\" \".join([target_int_to_letter[i] for i in answer_logits if i != pad])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Можно эту модель обучить предложения из четырёх слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "но уже не остаётся времени"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_text_split_sentences = []\n",
    "train_text_split_reversed_sentences = []\n",
    "\n",
    "orig_sentence = []\n",
    "rev_sentence = []\n",
    "for i in range(len(train_text_split)):\n",
    "  if i % 5 == 0:\n",
    "    train_text_split_sentences.append(' '.join(orig_sentence))\n",
    "    train_text_split_reversed_sentences.append(' '.join(rev_sentence))\n",
    "    orig_sentence = []\n",
    "    rev_sentence = []\n",
    "  else:\n",
    "    orig_sentence.append(train_text_split[i])\n",
    "    rev_sentence.append(train_text_split_reverse[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'stsihcrana etacovda laicos snoitaler', 'nopu yratnulov noitaicossa fo', 'slaudividni lautum dia dna', 'ecnanrevog elihw msihcrana si']\n",
      "['', 'anarchists advocate social relations', 'upon voluntary association of', 'individuals mutual aid and', 'governance while anarchism is']\n"
     ]
    }
   ],
   "source": [
    "print(train_text_split_reversed_sentences[:5])\n",
    "print(train_text_split_sentences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/text8original.txt', 'w') as f:\n",
    "  f.write('\\n'.join(train_text_split))\n",
    "with open('data/text8reversed.txt', 'w') as f:\n",
    "  f.write('\\n'.join(train_text_split_reverse))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
